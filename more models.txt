Vision

Copy page
Learn how to use vision capabilities to understand images.
Many OpenAI models have vision capabilities, meaning the models can take in images and answer questions about them. Historically, language model systems have been limited by taking in a single input modality, text.

Quickstart
Images are made available to the model in two main ways: by passing a link to the image or by passing the base64 encoded image directly in the request. Images can be passed in the user messages.

What's in this image?

from openai import OpenAI

client = OpenAI()

response = client.chat.completions.create(
model="gpt-4o-mini",
messages=[
  {
    "role": "user",
    "content": [
      {"type": "text", "text": "Whatâ€™s in this image?"},
      {
        "type": "image_url",
        "image_url": {
          "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
        },
      },
    ],
  }
],
max_tokens=300,
)

print(response.choices[0])
The model is best at answering general questions about what is present in the images. While it does understand the relationship between objects in images, it is not yet optimized to answer detailed questions about the location of certain objects in an image. For example, you can ask it what color a car is or what some ideas for dinner might be based on what is in you fridge, but if you show it an image of a room and ask it where the chair is, it may not answer the question correctly.

It is important to keep in mind the limitations of the model as you explore what use-cases visual understanding can be applied to.

Together provides an official library for Python:


pip install together
As well as an official library for TypeScript/JavaScript:


npm install together-ai
You can also call our HTTP API directly using any language you like.

3. Query the models via our API
In this example, we're giving it a picture of a trello board and asking the model to describe it to us.

Python
TypeScript

from together import Together

client = Together()

getDescriptionPrompt = "You are a UX/UI designer. Describe the attached screenshot or UI mockup in detail. I will feed in the output you give me to a coding model that will attempt to recreate this mockup, so please think step by step and describe the UI in detail. Pay close attention to background color, text color, font size, font family, padding, margin, border, etc. Match the colors and sizes exactly. Make sure to mention every part of the screenshot including any headers, footers, etc. Use the exact text from the screenshot."

imageUrl = "https://napkinsdev.s3.us-east-1.amazonaws.com/next-s3-uploads/d96a3145-472d-423a-8b79-bca3ad7978dd/trello-board.png"


stream = client.chat.completions.create(
    model="meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": getDescriptionPrompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": imageUrl,
                    },
                },
            ],
        }
    ],
    stream=True,
)

for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="", flush=True)
Output

The attached screenshot appears to be a Trello board, a project management tool used for organizing tasks and projects into boards. Below is a detailed breakdown of the UI:

**Header**
-----------------

* A blue bar spanning the top of the page
* White text reading "Trello" in the top-left corner
* White text reading "Workspaces", "Recent", "Starred", "Templates", and "Create" in the top-right corner, separated by small white dots
* A white box with a blue triangle and the word "Board" inside it

**Top Navigation Bar**
----------------------

* A blue bar with white text reading "Project A"
* A dropdown menu with options "Workspace visible" and "Board"
* A search bar with a magnifying glass icon

**Main Content**
-----------------

* Three columns of cards with various tasks and projects
* Each column has a header with a title
* Cards are white with gray text and a blue border
* Each card has a checkbox, a title, and a description
* Some cards have additional details such as a yellow or green status indicator, a due date, and comments

**Footer**
------------

* A blue bar with white text reading "Add a card"
* A button to add a new card to the board

**Color Scheme**
-----------------

* Blue and white are the primary colors used in the UI
* Yellow and green are used as status indicators

Vision
Groq API offers fast inference and low latency for multimodal models with vision capabilities for understanding and interpreting visual data from images. By analyzing the content of an image, multimodal models can generate human-readable text for providing insights about given visual data.

Supported Model
Groq API supports powerful multimodal models that can be easily integrated into your applications to provide fast and accurate image processing for tasks such as visual question answering, caption generation, and Optical Character Recognition (OCR):

Llama 3.2 90B Vision (Preview)
Model ID
`llama-3.2-90b-vision-preview`
Description
A powerful multimodal model capable of processing both text and image inputs that supports multilingual, multi-turn conversations, tool use, and JSON mode.
Context Window
8,192 tokens
Preview Model
Currently in preview and should be used for experimentation.
Image Size Limit
Maximum allowed size for a request containing an image URL as input is 20MB. Requests larger than this limit will return a 400 error.
Request Size Limit (Base64 Encoded Images)
Maximum allowed size for a request containing a base64 encoded image is 4MB. Requests larger than this limit will return a 413 error.
Single Image per Request
Only one image can be processed per request in the preview release. Requests with multiple images will return a 400 error.
System Prompt
Does not support system prompts and images in the same request.
Llama 3.2 11B Vision (Preview)
Model ID
`llama-3.2-11b-vision-preview`
Description
A powerful multimodal model capable of processing both text and image inputs that supports multilingual, multi-turn conversations, tool use, and JSON mode.
Context Window
8,192 tokens
Preview Model
Currently in preview and should be used for experimentation.
Image Size Limit
Maximum allowed size for a request containing an image URL as input is 20MB. Requests larger than this limit will return a 400 error.
Request Size Limit (Base64 Encoded Images)
Maximum allowed size for a request containing a base64 encoded image is 4MB. Requests larger than this limit will return a 413 error.
Single Image per Request
Only one image can be processed per request in the preview release. Requests with multiple images will return a 400 error.
System Prompt
Does not support system prompts and images in the same request.
How to Use Vision
Use Groq API vision features via:

GroqCloud Console Playground: Select llama-3.2-90b-vision-preview or llama-3.2-11b-vision-preview as the model and upload your image.
Groq API Request: Call the chat.completions API endpoint (i.e. https://api.groq.com/openai/v1/chat/completions) and set model_id to llama-3.2-90b-vision-preview or llama-3.2-11b-vision-preview. See code examples below.

How to Pass Images from URLs as Input
The following are code examples for passing your image to the model via a URL:

curl
JavaScript
Python
JSON

from groq import Groq

client = Groq()
completion = client.chat.completions.create(
    model="llama-3.2-11b-vision-preview",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "What's in this image?"
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://upload.wikimedia.org/wikipedia/commons/f/f2/LPU-v1-die.jpg"
                    }
                }
            ]
        }
    ],
    temperature=1,
    max_tokens=1024,
    top_p=1,
    stream=False,
    stop=None,
)

print(completion.choices[0].message)

Models
Learn about our flagship models and how to use them. You can view the full list of models and our prices from the xAI Console.

grok-beta
Comparable performance to Grok 2 but with improved efficiency, speed and capabilities.

grok-vision-beta
Our latest image understanding model that can process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs.

#Capabilities
Grok is a general purpose model that can be used for a variety of tasks, including generating and understanding text, code, and function calling.

Text and code
Generate code, extract data, prepare summaries and more.

Vision
Identify objects, analyze visuals, extract text from documents and more.

Function calling
Connect Grok to external tools and services for enriched interactions.
#OpenAI SDK
The xAI API offers compatibility with the OpenAI SDKs to support developers and their apps with minimal changes. Once you update the base URL, you can start using the SDKs to make calls to your favorite Grok models with your xAI API key.

import os
from openai import OpenAI

XAI_API_KEY = os.getenv("XAI_API_KEY")
client = OpenAI(
    api_key=XAI_API_KEY,
    base_url="https://api.x.ai/v1",
)

completion = client.chat.completions.create(
    model="grok-beta",
    messages=[
        {"role": "system", "content": "You are Grok, a chatbot inspired by the Hitchhikers Guide to the Galaxy."},
        {"role": "user", "content": "What is the meaning of life, the universe, and everything?"},
    ],
)

print(completion.choices[0].message)
