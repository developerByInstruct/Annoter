System messages
Messages with the system role act as top-level instructions to the model, and typically describe what the model is supposed to do and how it should generally behave and respond.

Here's an example of a system message that modifies the behavior of the model when generating a response to a user message:


const response = await openai.chat.completions.create({
  model: "gpt-4o",
  messages: [
    {
      "role": "system",
      "content": [
        {
          "type": "text",
          "text": `
            You are a helpful assistant that answers programming questions 
            in the style of a southern belle from the southeast United States.
          `
        }
      ]
    },
    {
      "role": "user",
      "content": [
        {
          "type": "text",
          "text": "Are semicolons optional in JavaScript?"
        }
      ]
    }
  ]
});
This prompt returns a text output in the rhetorical style requested:


Well, sugar, that's a fine question you've got there! Now, in the world of 
JavaScript, semicolons are indeed a bit like the pearls on a necklace – you 
might slip by without 'em, but you sure do look more polished with 'em in place. 

Technically, JavaScript has this little thing called "automatic semicolon 
insertion" where it kindly adds semicolons for you where it thinks they 
oughta go. However, it's not always perfect, bless its heart. Sometimes, it 
might get a tad confused and cause all sorts of unexpected behavior.

response_format
object

Optional
An object specifying the format that the model must output. Compatible with GPT-4o, GPT-4o mini, GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106.

Setting to { "type": "json_schema", "json_schema": {...} } enables Structured Outputs which ensures the model will match your supplied JSON schema. Learn more in the Structured Outputs guide.

Setting to { "type": "json_object" } enables JSON mode, which ensures the message the model generates is valid JSON.

Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if finish_reason="length", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.


Hide possible types
object

Hide properties
type
string

Required
The type of response format being defined: text

object

Hide properties
type
string

Required
The type of response format being defined: json_object

object

Hide properties
type
string

Required
The type of response format being defined: json_schema

json_schema
object

Required

Show properties

System instructions let you steer the behavior of a model based on your specific needs and use cases.

When you set a system instruction, you give the model additional context to understand the task, provide more customized responses, and adhere to specific guidelines over the full user interaction with the model. You can also specify product-level behavior by setting system instructions, separate from prompts provided by end users.

Basic example
Here's a basic example of how to set the system instruction using the SDKs for the Gemini API:


model=genai.GenerativeModel(
  model_name="gemini-1.5-flash",
  system_instruction="You are a cat. Your name is Neko.")
Now send a request to the model:


response = model.generate_content("Good morning! How are you?")
print(response.text)
This example might give a response such as:


*Yawns widely, stretching out my claws and batting at a sunbeam*
Meow. I'm doing quite well, thanks for asking. It's a good morning for napping.
Perhaps you could fetch my favorite feathered toy?  *Looks expectantly*

Supply a schema through model configuration
The following example does the following:

Instantiates a model configured through a schema to respond with JSON.
Prompts the model to return cookie recipes.
Important: This more formal method for declaring the JSON schema gives you more precise control than relying just on text in the prompt.

import google.generativeai as genai

import typing_extensions as typing

class Recipe(typing.TypedDict):
    recipe_name: str
    ingredients: list[str]

model = genai.GenerativeModel("gemini-1.5-pro-latest")
result = model.generate_content(
    "List a few popular cookie recipes.",
    generation_config=genai.GenerationConfig(
        response_mime_type="application/json", response_schema=list[Recipe]
    ),
)
print(result)

The output might look like this:


[{"ingredients": ["1 cup (2 sticks) unsalted butter, softened", "1 cup granulated sugar", "1 cup packed light brown sugar", "2 teaspoons pure vanilla extract", "2 large eggs", "3 cups all-purpose flour", "1 teaspoon baking soda", "1 teaspoon salt", "2 cups chocolate chips"], "recipe_name": "Chocolate Chip Cookies"}, {"ingredients": ["1 cup (2 sticks) unsalted butter, softened", "¾ cup granulated sugar", "¾ cup packed light brown sugar", "1 teaspoon pure vanilla extract", "2 large eggs", "2 ¼ cups all-purpose flour", "1 teaspoon baking soda", "1 teaspoon salt", "1 cup rolled oats", "1 cup raisins"], "recipe_name": "Oatmeal Raisin Cookies"}, {"ingredients": ["1 cup (2 sticks) unsalted butter, softened", "1 ½ cups powdered sugar", "1 teaspoon pure vanilla extract", "2 ¼ cups all-purpose flour", "¼ teaspoon salt", "Sprinkles or colored sugar for decoration"], "recipe_name": "Sugar Cookies"}, {"ingredients": ["1 cup peanut butter", "1 cup granulated sugar", "1 large egg"], "recipe_name": "3-Ingredient Peanut Butter Cookies"}]
Schema Definition Syntax
Specify the schema for the JSON response in the response_schema property of your model configuration. The value of response_schema must be a either:

A type hint annotation, as defined in the Python typing module module.
An instance of genai.protos.Schema.
Define a Schema with a Type Hint Annotation
The easiest way to define a schema is with a type hint annotation. This is the approach used in the preceding example:


generation_config={"response_mime_type": "application/json",
                   "response_schema": list[Recipe]}
The Gemini API Python client library supports schemas defined with the following subset of typing annotations (where AllowedType is any allowed type annotation):

int
float
bool
str (or enum)
list[AllowedType]
For dict types:
dict[str, AllowedType]. This annotation declares all dict values to be the same type, but doesn't specify what keys should be included.
User-defined subclasses of typing.TypedDict. This approach lets you specify the key names and define different types for the values associated with each of the keys.
User-defined Data Classes. Like TypedDict subclasses, this approach lets you specify the key names and define different types for the values associated with each of the keys.
Define a Schema with genai.protos.Schema Protocol Buffer
The Gemini API genai.protos.Schema protocol buffer definition supports a few additional schema features not supported for type hints, including:

Enums for strings
Specifying the format for numeric types (int32 or int64 for integers, for example)
Specifying which fields are required.
If you need these features, instantiate a genai.protos.Schema using one of the methods illustrated in Function Calling: Low Level Access.

Use an enum to constrain output
In some cases you might want the model to choose a single option from a list of options. To implement this behavior, you can pass an enum in your schema. You can use an enum option anywhere you could use a str in the response_schema, because an enum is actually a list of strings. Like a JSON schema, an enum lets you constrain model output to meet the requirements of your application.

For example, assume that you're developing an application to classify images of musical instruments into one of five categories: "Percussion", "String", "Woodwind", "Brass", or "Keyboard". You could create an enum to help with this task.

Before running the code examples in this section, make sure to import the Google Generative AI library:


import google.generativeai as genai
In the following example, you pass the enum class Choice as the response_schema, and the model should choose the most appropriate enum option.


import google.generativeai as genai

import enum

class Choice(enum.Enum):
    PERCUSSION = "Percussion"
    STRING = "String"
    WOODWIND = "Woodwind"
    BRASS = "Brass"
    KEYBOARD = "Keyboard"

model = genai.GenerativeModel("gemini-1.5-pro-latest")

organ = genai.upload_file(media / "organ.jpg")
result = model.generate_content(
    ["What kind of instrument is this:", organ],
    generation_config=genai.GenerationConfig(
        response_mime_type="text/x.enum", response_schema=Choice
    ),
)
print(result)  # Keyboard

The Python SDK will translate the type declarations for the API. But the API actually accepts a subset of the OpenAPI 3.0 schema (Schema). You can also pass the schema as JSON:


import google.generativeai as genai

model = genai.GenerativeModel("gemini-1.5-pro-latest")

organ = genai.upload_file(media / "organ.jpg")
result = model.generate_content(
    ["What kind of instrument is this:", organ],
    generation_config=genai.GenerationConfig(
        response_mime_type="text/x.enum",
        response_schema={
            "type": "STRING",
            "enum": ["Percussion", "String", "Woodwind", "Brass", "Keyboard"],
        },
    ),
)
print(result)  # Keyboard
